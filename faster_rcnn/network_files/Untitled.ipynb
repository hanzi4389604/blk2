{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95565fc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pair\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _assert_has_ops\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_boxes_to_roi_format, check_roi_boxes_shape\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroi_align\u001b[39m(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m     15\u001b[0m     boxes: Union[Tensor, List[Tensor]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     aligned: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.jit.annotations import BroadcastingList2\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torchvision.extension import _assert_has_ops\n",
    "\n",
    "from ..utils import _log_api_usage_once\n",
    "from ._utils import convert_boxes_to_roi_format, check_roi_boxes_shape\n",
    "\n",
    "\n",
    "def roi_align(\n",
    "    input: Tensor,\n",
    "    boxes: Union[Tensor, List[Tensor]],\n",
    "    output_size: BroadcastingList2[int],\n",
    "    spatial_scale: float = 1.0,\n",
    "    sampling_ratio: int = -1,\n",
    "    aligned: bool = False,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Performs Region of Interest (RoI) Align operator with average pooling, as described in Mask R-CNN.\n",
    "\n",
    "    Args:\n",
    "        input (Tensor[N, C, H, W]): The input tensor, i.e. a batch with ``N`` elements. Each element\n",
    "            contains ``C`` feature maps of dimensions ``H x W``.\n",
    "            If the tensor is quantized, we expect a batch size of ``N == 1``.\n",
    "        boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2)\n",
    "            format where the regions will be taken from.\n",
    "            The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``.\n",
    "            If a single Tensor is passed, then the first column should\n",
    "            contain the index of the corresponding element in the batch, i.e. a number in ``[0, N - 1]``.\n",
    "            If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i\n",
    "            in the batch.\n",
    "        output_size (int or Tuple[int, int]): the size of the output (in bins or pixels) after the pooling\n",
    "            is performed, as (height, width).\n",
    "        spatial_scale (float): a scaling factor that maps the box coordinates to\n",
    "            the input coordinates. For example, if your boxes are defined on the scale\n",
    "            of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of\n",
    "            the original image), you'll want to set this to 0.5. Default: 1.0\n",
    "        sampling_ratio (int): number of sampling points in the interpolation grid\n",
    "            used to compute the output value of each pooled output bin. If > 0,\n",
    "            then exactly ``sampling_ratio x sampling_ratio`` sampling points per bin are used. If\n",
    "            <= 0, then an adaptive number of grid points are used (computed as\n",
    "            ``ceil(roi_width / output_width)``, and likewise for height). Default: -1\n",
    "        aligned (bool): If False, use the legacy implementation.\n",
    "            If True, pixel shift the box coordinates it by -0.5 for a better alignment with the two\n",
    "            neighboring pixel indices. This version is used in Detectron2\n",
    "\n",
    "    Returns:\n",
    "        Tensor[K, C, output_size[0], output_size[1]]: The pooled RoIs.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
    "        _log_api_usage_once(roi_align)\n",
    "    _assert_has_ops()\n",
    "    check_roi_boxes_shape(boxes)\n",
    "    rois = boxes\n",
    "    output_size = _pair(output_size)\n",
    "    if not isinstance(rois, torch.Tensor):\n",
    "        rois = convert_boxes_to_roi_format(rois)\n",
    "    return torch.ops.torchvision.roi_align(\n",
    "        input, rois, spatial_scale, output_size[0], output_size[1], sampling_ratio, aligned\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class RoIAlign(nn.Module):\n",
    "    \"\"\"\n",
    "    See :func:`roi_align`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size: BroadcastingList2[int],\n",
    "        spatial_scale: float,\n",
    "        sampling_ratio: int,\n",
    "        aligned: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.output_size = output_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "        self.aligned = aligned\n",
    "\n",
    "    def forward(self, input: Tensor, rois: Tensor) -> Tensor:\n",
    "        return roi_align(input, rois, self.output_size, self.spatial_scale, self.sampling_ratio, self.aligned)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        s = (\n",
    "            f\"{self.__class__.__name__}(\"\n",
    "            f\"output_size={self.output_size}\"\n",
    "            f\", spatial_scale={self.spatial_scale}\"\n",
    "            f\", sampling_ratio={self.sampling_ratio}\"\n",
    "            f\", aligned={self.aligned}\"\n",
    "            f\")\"\n",
    "        )\n",
    "        return s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
