{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87eb7f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (471943596.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    [docs]def roi_align(\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.jit.annotations import BroadcastingList2\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torchvision.extension import _assert_has_ops\n",
    "\n",
    "#from ..utils import _log_api_usage_once\n",
    "from ._utils import convert_boxes_to_roi_format, check_roi_boxes_shape\n",
    "\n",
    "\n",
    "def roi_align(\n",
    "    input: Tensor,\n",
    "    boxes: Union[Tensor, List[Tensor]],\n",
    "    output_size: BroadcastingList2[int],\n",
    "    spatial_scale: float = 1.0,\n",
    "    sampling_ratio: int = -1,\n",
    "    aligned: bool = False,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Performs Region of Interest (RoI) Align operator with average pooling, as described in Mask R-CNN.\n",
    "\n",
    "    Args:\n",
    "        input (Tensor[N, C, H, W]): The input tensor, i.e. a batch with ``N`` elements. Each element\n",
    "            contains ``C`` feature maps of dimensions ``H x W``.\n",
    "            If the tensor is quantized, we expect a batch size of ``N == 1``.\n",
    "        boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2)\n",
    "            format where the regions will be taken from.\n",
    "            The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``.\n",
    "            If a single Tensor is passed, then the first column should\n",
    "            contain the index of the corresponding element in the batch, i.e. a number in ``[0, N - 1]``.\n",
    "            If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i\n",
    "            in the batch.\n",
    "        output_size (int or Tuple[int, int]): the size of the output (in bins or pixels) after the pooling\n",
    "            is performed, as (height, width).\n",
    "        spatial_scale (float): a scaling factor that maps the box coordinates to\n",
    "            the input coordinates. For example, if your boxes are defined on the scale\n",
    "            of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of\n",
    "            the original image), you'll want to set this to 0.5. Default: 1.0\n",
    "        sampling_ratio (int): number of sampling points in the interpolation grid\n",
    "            used to compute the output value of each pooled output bin. If > 0,\n",
    "            then exactly ``sampling_ratio x sampling_ratio`` sampling points per bin are used. If\n",
    "            <= 0, then an adaptive number of grid points are used (computed as\n",
    "            ``ceil(roi_width / output_width)``, and likewise for height). Default: -1\n",
    "        aligned (bool): If False, use the legacy implementation.\n",
    "            If True, pixel shift the box coordinates it by -0.5 for a better alignment with the two\n",
    "            neighboring pixel indices. This version is used in Detectron2\n",
    "\n",
    "    Returns:\n",
    "        Tensor[K, C, output_size[0], output_size[1]]: The pooled RoIs.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
    "        _log_api_usage_once(roi_align)\n",
    "    _assert_has_ops()\n",
    "    check_roi_boxes_shape(boxes)\n",
    "    rois = boxes\n",
    "    output_size = _pair(output_size)\n",
    "    if not isinstance(rois, torch.Tensor):\n",
    "        rois = convert_boxes_to_roi_format(rois)\n",
    "    return torch.ops.torchvision.roi_align(\n",
    "        input, rois, spatial_scale, output_size[0], output_size[1], sampling_ratio, aligned\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class RoIAlign(nn.Module):\n",
    "    \"\"\"\n",
    "    See :func:`roi_align`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size: BroadcastingList2[int],\n",
    "        spatial_scale: float,\n",
    "        sampling_ratio: int,\n",
    "        aligned: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.output_size = output_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "        self.aligned = aligned\n",
    "\n",
    "    def forward(self, input: Tensor, rois: Tensor) -> Tensor:\n",
    "        return roi_align(input, rois, self.output_size, self.spatial_scale, self.sampling_ratio, self.aligned)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        s = (\n",
    "            f\"{self.__class__.__name__}(\"\n",
    "            f\"output_size={self.output_size}\"\n",
    "            f\", spatial_scale={self.spatial_scale}\"\n",
    "            f\", sampling_ratio={self.sampling_ratio}\"\n",
    "            f\", aligned={self.aligned}\"\n",
    "            f\")\"\n",
    "        )\n",
    "        return s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
